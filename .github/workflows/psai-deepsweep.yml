name: PSAI DeepSweep Discovery

on:
  schedule:
    - cron: '45 13 * * *'  # 09:45 ET daily — runs after fullfield
  workflow_dispatch:
  push:
    paths:
      - 'scripts/**'
      - 'data/**'
      - '.github/workflows/psai-deepsweep.yml'

permissions:
  contents: write
  id-token: write
  pages: write

concurrency:
  group: psai-build
  cancel-in-progress: true

jobs:
  deepsweep:
    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 PyYAML

      - name: Discovery sweep (Product Hunt, GitHub, HF, Reddit, HN, press, registries, arXiv/PwC)
        env:
          PSAI_MAX_RESULTS: "300"
          PSAI_TIMEOUT_S: "45"
        run: |
          python scripts/discover.py             --product-hunt --github --huggingface --reddit --hn --press --registries --papers --arxiv             --out data/candidates.json

      - name: Merge & append new tools to tracker
        run: |
          python scripts/merge_candidates.py             --candidates data/candidates.json             --tracker data/PSAI_FullField_Tracker.csv             --out data/PSAI_FullField_Tracker.csv

      - name: Build feeds
        env:
          SITE_URL: ${{ vars.SITE_URL }}
        run: |
          SITE="${SITE_URL}"
          if [ -z "$SITE" ]; then
            SITE="https://${GITHUB_REPOSITORY_OWNER}.github.io/${GITHUB_REPOSITORY#*/}"
          fi
          python scripts/build_feed_from_log.py --site "$SITE"

      - name: Build sources pages (cards + table)
        env:
          PSAI_INCLUDE_REGEX: "(agent|agentic|orchestr|mcp|ide|editor|review|code\\s*assistant)"
          PSAI_EXCLUDE_REGEX: "^(product\\s*hunt|reddit|hacker\\s*news|hn\\s*—|hn\\s*show|latentspace|ben['’]s\\s*bites|npm|pypi|docker\\s*hub)\\b"
          PSAI_MIN_STARS: "50"
        run: python scripts/build_sources_pages.py

      - name: Commit discovery results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/candidates.json data/PSAI_FullField_Tracker.csv public/*.html public/*.json public/*.xml || true
          git commit -m "PSAI DeepSweep: discovery & tracker append $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || echo "No changes"
          git push || true
          
      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: public
          
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
